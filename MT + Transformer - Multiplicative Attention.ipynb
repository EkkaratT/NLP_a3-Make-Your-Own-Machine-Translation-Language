{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('kvush/english_thai_texts')\n",
    "\n",
    "ENG_LANGUAGE = 'input_text'\n",
    "THAI_LANGUAGE = 'translated_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the dataset manually into train, validation, and test\n",
    "\n",
    "train_subset = dataset['train'].select(range(41901))   # Select first 70% rows for training\n",
    "\n",
    "validation_subset = dataset['train'].select(range(41901, 47887))   # Select next 10% rows for validation\n",
    "\n",
    "test_subset = dataset['train'].select(range(47887, 59859))   # Select next 20% rows for testing\n",
    "\n",
    "# Create a new DatasetDict with the desired subsets\n",
    "dataset = DatasetDict({\n",
    "    'train': train_subset,\n",
    "    'validation': validation_subset,\n",
    "    'test': test_subset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 41901\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 5986\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 11972\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['validation']]\n",
    "test = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11972"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "token_transform[ENG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[THAI_LANGUAGE] = word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the English and Thai sentences\n",
    "english_sentence = sample[0]  # English part\n",
    "thai_sentence = sample[1]  # Thai part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence:  Service, scallops, all - top notch in every way!\n",
      "English Tokenization:  ['Service', ',', 'scallops', ',', 'all', '-', 'top', 'notch', 'in', 'every', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of English sentence\n",
    "print(\"English Sentence: \", english_sentence)\n",
    "print(\"English Tokenization: \", token_transform[ENG_LANGUAGE](english_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thai Sentence:  บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!\n",
      "Thai Tokenization:  ['บริการ', 'หอย', 'เชลล์', 'ทั้งหมด', ' ', '-', ' ', 'โดดเด่น', 'ใน', 'ทุก', 'ด้าน', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of Thai sentence\n",
    "print(\"Thai Sentence: \", thai_sentence)\n",
    "print(\"Thai Tokenization: \", token_transform[THAI_LANGUAGE](thai_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {ENG_LANGUAGE: 0, THAI_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 12, 10, 0, 10]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[ENG_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haircut'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[ENG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16607"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[ENG_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[THAI_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 43])\n",
      "Thai shape:  torch.Size([64, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim     #size of word embedding\n",
    "        self.n_heads  = n_heads      #head\n",
    "        self.head_dim = hid_dim // n_heads   #dk\n",
    "        \n",
    "        # Implement Multiplicative Attention \n",
    "        self.W = nn.Linear(self.head_dim, self.head_dim)  # for query (decoder) input\n",
    "        self.U = nn.Linear(self.head_dim, self.head_dim)  # for key (encoder) input\n",
    "        self.v = nn.Linear(self.head_dim, 1)  # for attention score\n",
    "\n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "\n",
    "        q_w = self.W(Q)  # [batch_size, n_heads, query_len, head_dim]\n",
    "\n",
    "        q_w = torch.matmul(Q, self.W)  # [batch_size, n_heads, query len, head_dim]\n",
    "\n",
    "        # Energy computation using additive scoring function (s^T W h_i)\n",
    "        energy = torch.matmul(q_w, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #for making attention to padding to 0 \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[ENG_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[THAI_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16607, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(11664, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=11664, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[ENG_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[THAI_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4251392\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 11664\n",
      "______\n",
      "14259193\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 54s\n",
      "\tTrain Loss: 4.942 | Train PPL: 140.094\n",
      "\t Val. Loss: 3.883 |  Val. PPL:  48.567\n",
      "Epoch: 02 | Time: 2m 31s\n",
      "\tTrain Loss: 3.413 | Train PPL:  30.369\n",
      "\t Val. Loss: 2.948 |  Val. PPL:  19.073\n",
      "Epoch: 03 | Time: 2m 25s\n",
      "\tTrain Loss: 2.644 | Train PPL:  14.070\n",
      "\t Val. Loss: 2.524 |  Val. PPL:  12.482\n",
      "Epoch: 04 | Time: 2m 21s\n",
      "\tTrain Loss: 2.160 | Train PPL:   8.668\n",
      "\t Val. Loss: 2.271 |  Val. PPL:   9.686\n",
      "Epoch: 05 | Time: 2m 21s\n",
      "\tTrain Loss: 1.824 | Train PPL:   6.198\n",
      "\t Val. Loss: 2.132 |  Val. PPL:   8.432\n",
      "Epoch: 06 | Time: 2m 27s\n",
      "\tTrain Loss: 1.575 | Train PPL:   4.832\n",
      "\t Val. Loss: 2.050 |  Val. PPL:   7.767\n",
      "Epoch: 07 | Time: 2m 26s\n",
      "\tTrain Loss: 1.389 | Train PPL:   4.011\n",
      "\t Val. Loss: 2.027 |  Val. PPL:   7.588\n",
      "Epoch: 08 | Time: 2m 26s\n",
      "\tTrain Loss: 1.242 | Train PPL:   3.462\n",
      "\t Val. Loss: 1.999 |  Val. PPL:   7.379\n",
      "Epoch: 09 | Time: 2m 18s\n",
      "\tTrain Loss: 1.122 | Train PPL:   3.070\n",
      "\t Val. Loss: 2.009 |  Val. PPL:   7.459\n",
      "Epoch: 10 | Time: 2m 21s\n",
      "\tTrain Loss: 1.021 | Train PPL:   2.777\n",
      "\t Val. Loss: 2.018 |  Val. PPL:   7.525\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}-multiplicative.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.025 | Test PPL:   7.580 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service, scallops, all - top notch in every way!'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  153,    6, 2910,    6,   64,   58,  508, 1441,   19,  250,  219,\n",
       "          11,    3], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[ENG_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   28, 1352, 2288,  168,    4,  150,    4,  661,   11,  326,  422,\n",
       "          13,    3], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[THAI_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 11664])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 11664])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11664])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1352, 2288,  168,    4,  150,    4,  164,   11,  326,  422,   13,    3,\n",
       "          13], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[THAI_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หอย\n",
      "เชลล์\n",
      "ทั้งหมด\n",
      " \n",
      "-\n",
      " \n",
      "สุดยอด\n",
      "ใน\n",
      "ทุก\n",
      "ด้าน\n",
      "!\n",
      "<eos>\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 14, 14])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Service',\n",
       " ',',\n",
       " 'scallops',\n",
       " ',',\n",
       " 'all',\n",
       " '-',\n",
       " 'top',\n",
       " 'notch',\n",
       " 'in',\n",
       " 'every',\n",
       " 'way',\n",
       " '!',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[ENG_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'หอย',\n",
       " 'เชลล์',\n",
       " 'ทั้งหมด',\n",
       " ' ',\n",
       " '-',\n",
       " ' ',\n",
       " 'สุดยอด',\n",
       " 'ใน',\n",
       " 'ทุก',\n",
       " 'ด้าน',\n",
       " '!',\n",
       " '<eos>',\n",
       " '!']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_11740\\59549304.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_11740\\59549304.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3629 (\\N{THAI CHARACTER O ANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3618 (\\N{THAI CHARACTER YO YAK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3648 (\\N{THAI CHARACTER SARA E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3594 (\\N{THAI CHARACTER CHO CHANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3660 (\\N{THAI CHARACTER THANTHAKHAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3591 (\\N{THAI CHARACTER NGO NGU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3617 (\\N{THAI CHARACTER MO MA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3604 (\\N{THAI CHARACTER DO DEK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3626 (\\N{THAI CHARACTER SO SUA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3640 (\\N{THAI CHARACTER SARA U}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3651 (\\N{THAI CHARACTER SARA AI MAIMUAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
