{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('kvush/english_thai_texts')\n",
    "\n",
    "ENG_LANGUAGE = 'input_text'\n",
    "THAI_LANGUAGE = 'translated_text'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the dataset manually into train, validation, and test\n",
    "\n",
    "train_subset = dataset['train'].select(range(41901))   # Select first 70% rows for training\n",
    "\n",
    "validation_subset = dataset['train'].select(range(41901, 47887))   # Select next 10% rows for validation\n",
    "\n",
    "test_subset = dataset['train'].select(range(47887, 59859))   # Select next 20% rows for testing\n",
    "\n",
    "# Create a new DatasetDict with the desired subsets\n",
    "dataset = DatasetDict({\n",
    "    'train': train_subset,\n",
    "    'validation': validation_subset,\n",
    "    'test': test_subset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 41901\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 5986\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 11972\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['validation']]\n",
    "test = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11972"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "token_transform[ENG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[THAI_LANGUAGE] = word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the English and Thai sentences\n",
    "english_sentence = sample[0]  # English part\n",
    "thai_sentence = sample[1]  # Thai part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence:  Service, scallops, all - top notch in every way!\n",
      "English Tokenization:  ['Service', ',', 'scallops', ',', 'all', '-', 'top', 'notch', 'in', 'every', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of English sentence\n",
    "print(\"English Sentence: \", english_sentence)\n",
    "print(\"English Tokenization: \", token_transform[ENG_LANGUAGE](english_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thai Sentence:  บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!\n",
      "Thai Tokenization:  ['บริการ', 'หอย', 'เชลล์', 'ทั้งหมด', ' ', '-', ' ', 'โดดเด่น', 'ใน', 'ทุก', 'ด้าน', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of Thai sentence\n",
    "print(\"Thai Sentence: \", thai_sentence)\n",
    "print(\"Thai Tokenization: \", token_transform[THAI_LANGUAGE](thai_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {ENG_LANGUAGE: 0, THAI_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 12, 10, 0, 10]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[ENG_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haircut'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[ENG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16607"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[ENG_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[THAI_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 43])\n",
      "Thai shape:  torch.Size([64, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Ensure hidden dimension is divisible by number of heads\n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim  = hid_dim     # Hidden dimension of the model (word embedding size)\n",
    "        self.n_heads  = n_heads     # Number of attention heads\n",
    "        self.head_dim = hid_dim // n_heads  # Dimension of each attention head (dk)\n",
    "\n",
    "        # Implement Additive Attention (using learnable transformations for query, key, and value)\n",
    "        self.W = nn.Linear(self.head_dim, self.head_dim)  # Transformation for query (decoder) input\n",
    "        self.U = nn.Linear(self.head_dim, self.head_dim)  # Transformation for key (encoder) input\n",
    "        self.v = nn.Linear(self.head_dim, 1)  # Weight vector for computing attention scores\n",
    "        \n",
    "        # Linear layers to project input (query, key, value) to appropriate dimensions\n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)  # For query input projection\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)  # For key input projection\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)  # For value input projection\n",
    "        \n",
    "        # Output projection to get final output after attention is applied\n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        # Scaling factor for attention (sqrt of head_dim)\n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Input shapes:\n",
    "        # query = [batch_size, query_len, hid_dim]\n",
    "        # key = [batch_size, key_len, hid_dim]\n",
    "        # value = [batch_size, value_len, hid_dim]\n",
    "        # mask = [batch_size, 1, query_len, key_len] (optional, used to mask padding tokens)\n",
    "\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Project the query, key, and value to the hidden dimension space\n",
    "        Q = self.fc_q(query)  # [batch_size, query_len, hid_dim]\n",
    "        K = self.fc_k(key)    # [batch_size, key_len, hid_dim]\n",
    "        V = self.fc_v(value)  # [batch_size, value_len, hid_dim]\n",
    "        \n",
    "        # Reshape Q, K, V to split the heads for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, n_heads, query_len, head_dim]\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, n_heads, key_len, head_dim]\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)  # [batch_size, n_heads, value_len, head_dim]\n",
    "\n",
    "        q_len = query.shape[1]  # Length of query (e.g., number of words in query)\n",
    "        k_len = key.shape[1]    # Length of key (e.g., number of words in key)\n",
    "\n",
    "        # Apply the additive attention mechanism (computing the energy term for additive attention)\n",
    "        q_w = self.W(Q).view(batch_size, self.n_heads, q_len, 1, self.head_dim)  # [batch_size, n_heads, q_len, 1, head_dim]\n",
    "        k_u = self.U(K).view(batch_size, self.n_heads, 1, k_len, self.head_dim)  # [batch_size, n_heads, 1, k_len, head_dim]\n",
    "        \n",
    "        # Compute energy using additive attention (tanh activation)\n",
    "        energy = torch.tanh(q_w + k_u)  # [batch_size, n_heads, q_len, k_len]\n",
    "\n",
    "        # Compute the attention scores by applying the weight vector v on the energy term\n",
    "        attention_scores = self.v(energy).squeeze(-1)  # [batch_size, n_heads, q_len, k_len]\n",
    "\n",
    "        # Apply the mask to the attention scores (if provided) to ignore padding tokens\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        # Normalize the attention scores with softmax to obtain attention weights\n",
    "        attention = torch.softmax(attention_scores, dim=-1)  # [batch_size, n_heads, q_len, k_len]\n",
    "        \n",
    "        # Apply the attention weights to the value vectors (V) to get the final output\n",
    "        x = torch.matmul(self.dropout(attention), V)  # [batch_size, n_heads, q_len, head_dim]\n",
    "        \n",
    "        # Reshape the output to merge the attention heads\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # [batch_size, q_len, n_heads, head_dim]\n",
    "        x = x.view(batch_size, -1, self.hid_dim)  # [batch_size, q_len, hid_dim]\n",
    "        \n",
    "        # Final output projection after attention\n",
    "        x = self.fc_o(x)  # [batch_size, q_len, hid_dim]\n",
    "        \n",
    "        return x, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[ENG_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[THAI_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16607, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(11664, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (v): Linear(in_features=32, out_features=1, bias=True)\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=11664, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[ENG_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[THAI_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4251392\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 11664\n",
      "______\n",
      "14259193\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 59s\n",
      "\tTrain Loss: 4.951 | Train PPL: 141.372\n",
      "\t Val. Loss: 3.984 |  Val. PPL:  53.720\n",
      "Epoch: 02 | Time: 0m 59s\n",
      "\tTrain Loss: 3.721 | Train PPL:  41.320\n",
      "\t Val. Loss: 3.516 |  Val. PPL:  33.637\n",
      "Epoch: 03 | Time: 0m 59s\n",
      "\tTrain Loss: 3.209 | Train PPL:  24.767\n",
      "\t Val. Loss: 3.242 |  Val. PPL:  25.580\n",
      "Epoch: 04 | Time: 0m 59s\n",
      "\tTrain Loss: 2.674 | Train PPL:  14.494\n",
      "\t Val. Loss: 2.661 |  Val. PPL:  14.306\n",
      "Epoch: 05 | Time: 0m 59s\n",
      "\tTrain Loss: 2.112 | Train PPL:   8.262\n",
      "\t Val. Loss: 2.377 |  Val. PPL:  10.771\n",
      "Epoch: 06 | Time: 0m 59s\n",
      "\tTrain Loss: 1.772 | Train PPL:   5.884\n",
      "\t Val. Loss: 2.227 |  Val. PPL:   9.276\n",
      "Epoch: 07 | Time: 0m 59s\n",
      "\tTrain Loss: 1.533 | Train PPL:   4.633\n",
      "\t Val. Loss: 2.161 |  Val. PPL:   8.682\n",
      "Epoch: 08 | Time: 0m 59s\n",
      "\tTrain Loss: 1.355 | Train PPL:   3.876\n",
      "\t Val. Loss: 2.123 |  Val. PPL:   8.357\n",
      "Epoch: 09 | Time: 0m 59s\n",
      "\tTrain Loss: 1.219 | Train PPL:   3.385\n",
      "\t Val. Loss: 2.127 |  Val. PPL:   8.388\n",
      "Epoch: 10 | Time: 0m 59s\n",
      "\tTrain Loss: 1.108 | Train PPL:   3.027\n",
      "\t Val. Loss: 2.148 |  Val. PPL:   8.565\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}-addictive-att.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.139 | Test PPL:   8.489 |\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/{model.__class__.__name__}-addictive-att.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service, scallops, all - top notch in every way!'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  153,    6, 2910,    6,   64,   58,  508, 1441,   19,  250,  219,\n",
       "          11,    3], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[ENG_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   28, 1352, 2288,  168,    4,  150,    4,  661,   11,  326,  422,\n",
       "          13,    3], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[THAI_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 11664])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 11664])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11664])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1352, 2288,    4,    4,  150,    4,  164,   11,  326,  422,   13,    3,\n",
       "           3], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[THAI_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หอย\n",
      "เชลล์\n",
      " \n",
      " \n",
      "-\n",
      " \n",
      "สุดยอด\n",
      "ใน\n",
      "ทุก\n",
      "ด้าน\n",
      "!\n",
      "<eos>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 14, 14])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Service',\n",
       " ',',\n",
       " 'scallops',\n",
       " ',',\n",
       " 'all',\n",
       " '-',\n",
       " 'top',\n",
       " 'notch',\n",
       " 'in',\n",
       " 'every',\n",
       " 'way',\n",
       " '!',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[ENG_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'หอย',\n",
       " 'เชลล์',\n",
       " ' ',\n",
       " ' ',\n",
       " '-',\n",
       " ' ',\n",
       " 'สุดยอด',\n",
       " 'ใน',\n",
       " 'ทุก',\n",
       " 'ด้าน',\n",
       " '!',\n",
       " '<eos>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_5736\\3489918703.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_5736\\3489918703.py:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3629 (\\N{THAI CHARACTER O ANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3618 (\\N{THAI CHARACTER YO YAK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3648 (\\N{THAI CHARACTER SARA E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3594 (\\N{THAI CHARACTER CHO CHANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3660 (\\N{THAI CHARACTER THANTHAKHAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3626 (\\N{THAI CHARACTER SO SUA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3640 (\\N{THAI CHARACTER SARA U}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3604 (\\N{THAI CHARACTER DO DEK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3651 (\\N{THAI CHARACTER SARA AI MAIMUAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAANVCAYAAACZFHS8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfuklEQVR4nO3deXhU9dn44SdsAQ0ExA0BBbUiasUFK4qvYN2o1YpoFWtFtIrWWnCBCmgtrmjVirVurVVcqOKuxd26F/sCKnUpCi4IiksBSUAkLPn+/uDHvEaw9SjDSeJ9X1euNnNmwnOcZOZ85pw5U5JSSgEAAMBX0iDvAQAAAOoSEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAatFSuk/fg8AUF+IKOAbq66ujpKSkoiImDFjRqSUCt8DANQ3Igr4Rqqrq6NBg+UPJeeee26cccYZ8fTTT+c8FQBA8Ygo4BtZEVCnn356/P73v48+ffpE586da1ynuro6j9EAAIqiUd4DwH/isLC64aGHHorbbrstHn/88ejSpUssW7YsPv7443j99ddjl112icaNG9fYYwUAUJeJKGqNFcE0Z86cWLZsWbRq1SoaN26c91h8BcuWLYuWLVtG27Zt4/XXX4+//OUvcdNNN0V1dXWsu+668Y9//COaNGmS95gAAKuFl4WpFVYE1P333x+9evWKHj16xA477BCXXXZZvPfee3mPx+es6tC8Ro0aRXV1dRx66KHRs2fPmDlzZgwdOjRGjx4dH3/8cTz55JM5TAoAUBz2RFErlJSUxKOPPho/+clPYsSIEfHTn/40RowYEb/5zW+iU6dO0a5du7xHJGqeRGLGjBmxaNGi2GKLLaJXr16RUoqJEyfGL37xi+jRo0esu+66MWvWrFhvvfVi7bXXznlyAIDVpyT5MBdyllKK6urqOProo2ODDTaIiy++OD7++OPYddddY++9946rr746IiKWLFni8L5aYtiwYTF27NiYPXt2dO7cOY4++ujo169frLXWWhERsXTp0qioqIj+/fvHvHnz4qmnnoqGDRvmPDUAwOrhcD5yV1JSEg0bNowPPvgg9t5775g3b15sv/32seeeexYC6p577olXXnkl50m/vT5/CN/o0aPjxhtvjAsuuCDGjRsX3/nOd2L06NFx7rnnxqeffhoppbjiiivipz/9aXz44YfxxBNPRMOGDWPZsmU5rgEA8G2wqv1DxThLsIiqY+rLjsMV6zFv3rzCZWuvvXZcdNFFscMOO0Tv3r3jD3/4Q0REfPrppzFmzJh46qmnnCo7JysO4XvwwQdj7ty5ceaZZ0bfvn1j9913jxtuuCH22WefeOSRR+Kpp56KkpKSaNOmTfTo0SOef/75aNy4cSxdutSeKACgaFZsW644q/O8efNi8uTJERFFOTuww/nqiC+e6vvjjz+Ojz76KJo1axabb755jpNlt2JdHnroofjzn/8cxx57bPTq1SueeeaZGDBgQKSU4o033ihc/4wzzohbb701Hnvssdhss81ynPzb7YMPPoj27dtHdXV1/OpXv4oLL7ywxu9lt27domPHjnHrrbfWuN2yZcsEFABQNJ/fHlm6dGn8+c9/jnHjxsUDDzwQV155Zfz85z9f7f+mPVF1QHV1deEXo6qqKq6++uo48sgjY4899qiTZz0rKSmJe++9N/r06RM77bRTtGrVKiIitt9++zj++ONjyZIlseuuu8bxxx8fhxxySFxzzTVx1113Cag17Iuvr7Rp0yYmTJgQHTt2jGeeeSbef//9Gst79uwZ8+fPjyVLltS4XEAB1E8fffRR3iNARCzftly4cGH85je/if333z9GjBgRG2ywQbRv3z623377ovybIqoOaNCgQXz22WcxbNiw6NOnT5xzzjmx0UYbRZMmTaJTp055j/eVfH6D/L333otf//rXccEFF8Tpp58eO++8c0RENG/ePI477ri45ZZbomPHjjFv3rzYbLPNYvz48UX7A2DVPh/uy5YtK9x/O+ywQ9x6660xderUOPHEE2PatGmxaNGi+Oyzz+LJJ5+M1q1bO/kHwLfArbfeGqeddlpUVVU51J5cTZo0KUaOHBlbb711PP7447H77rvHu+++GyUlJdGxY8fCdubq5hTntdzf//73ePbZZ+Oaa66JNm3axMEHHxx33XVXDBkyJDp16hS777573iP+R5deemnstdde0aVLl8Jlc+bMiYqKihqzr9gNW1ZWFrvuumvsuuuueYxL1DyN+ahRo+LFF1+Mt99+O/r27Rs9evSI733ve/HAAw/E/vvvH3vttVd06tQpysvLY8mSJfGnP/0pIlY+/BQg4v8eG5YtWxYNGjSIkpKSGo85n78OtdusWbPi8ccfj4ULF0arVq3cb+Ti3nvvjV/+8pex0047xXHHHRfDhg2LiIjJkyfHhAkT4rrrrlvl48zqYE9ULZVSivHjx8f//M//xOTJk+OEE06I559/PgYPHhxTp06Nv//973HuuedGRNTas55NmzYtJkyYEM2aNatx+YpXrD5/UokVnnjiiXj44YcL33vL3pqz4r/1igeZoUOHxrnnnhvt27ePDTfcMEaPHh0DBw6Mf/zjH7HzzjvHQw89FE2aNImpU6fGqaeeGpMmTYomTZrEkiVLPJECq/Tqq69GxPJDb1Z8PuBRRx0VJ510Utxxxx2FZR77a68V2xynnXZabLrppvHrX/86IsLjPrnYdddd47bbbosbbrghhg8fXuOxZf311y98zmgxTiwhomqpkpKS2HXXXWPChAlx/fXXx9ChQwvLHnjggWjevHlsuummEVF733Pyne98J/785z/HFltsEePHj48XX3wxIiI6dOgQpaWlceWVV8bcuXMj4v8efB944IEYO3ZsfPbZZzUup/hWvFITsfwVnHvuuSfuvvvuOP/88+POO++M8847L9ZZZ50499xzY8aMGdG1a9cYO3ZsLFiwIH73u9/FZ599FtXV1Q7nA1bpySefjC5dusT1118fDRo0iIcffjgOOOCA+Oyzz+K1116LAQMGxEUXXRQRQqo2W7HNsWzZsjjggAPiX//6VyxYsCAivPDJmjN9+vT44IMPYv3114/u3btHeXl5Ydlrr70WF1xwQfTr1y/atGlTvCEStc4777yTPv7441UumzJlSmrdunW6+eab1/BU2SxdurTw/ysqKlKvXr3SlltumSZNmpRSSun5559PZWVl6YADDkh33313evLJJ9PJJ5+cWrRokV599dW8xv5WOumkk9IFF1xQ47IJEyakli1bpv/93/+tcfndd9+dOnToUOPyCRMmpA033DDtvffeqaKiYo3MDNQ97777bjrttNNSq1at0g033JBuueWWdOWVV6aUUvrwww/T7373u1RSUpJGjhxZuE11dXVe4/IFN954Y+rSpUt67LHH0nvvvZdSSum9995LrVq1ShdddFHO0/Ftcu+996Zu3bql3//+92nBggWFy5ctW5ZSSuniiy9OBx10UJo/f35R57Anqpa57777Yr/99otHH320xuFu6f+/uvPoo4/G//zP/8R+++2X04RfzYpXqt54441Ye+2141e/+lVsueWWccIJJ8SkSZOiW7duMX78+HjvvfdiyJAh8bOf/Syee+65ePrpp2PrrbfOefpvj1mzZsWiRYvipptuKnwuV0REkyZNYr311osZM2ZExP/9/h100EGRUoqnn366cN2ddtop7r777njrrbeisrJyza4AUGdsvPHGccopp8Rxxx0XJ598cowYMSLWX3/9iIjYYIMN4thjj41LL700hg8fHr/97W8jwtEItcW5554bTZo0iQ4dOsTpp58eP/zhD+PGG2+M0tLSOP/88+O5556LmTNn5j0m3wL33Xdf9O3bN/r27Rt9+vSJtddeu7CsQYMGsWzZsrjttttiyy23jLKysuIOU9REI5P77rsvrb322unSSy9NM2bMWGn5woULU/v27dNpp52Ww3TZvfnmm2n77bdPjz32WEoppYcffjgdcMABqWvXrmnChAkppeV7qaZPn56mTZuW5s6dm+e43zpLlixJKaX0+uuvp8GDB6dOnTql3//+94XlBx10UGrfvn168cUXC5fNnj07denSJf3lL39Z6ed99tlnxR8aqJNWvEKcUkoffPBBOuOMM1JpaWm6+OKLa1yvsrIyjRo1KpWUlKTLLrtsDU/Jqlx33XWppKQkvfDCCymllP7+97+nESNGpLZt26Z99tkndenSJW222Wbp2WefTSnV/b2HdX3++uyDDz5IO+20U2FbZdGiRWn27NnpjjvuKGyrzJkzJw0ZMiRVVVWllIp7f4qoWmLOnDlp5513Tuedd15Kafkvxty5c9Ptt9+ennnmmcL1Pr/rsrb/oS9cuDBtu+226Ygjjihc9uijjxZCasWhfax5p556atpss80KDzKvv/56Ou2001KnTp3S7373u8L1evTokTbaaKM0dOjQdNlll6W99947bbvttoUAA/hvVjxXTZo0KT355JOpqqoqffDBB2nw4MGpSZMm6YYbbqhx/YqKinTVVVelf/3rXzlMy+c98sgj6bzzzkt33HHHSsteffXVdNNNN6Vdd901lZSUpN133z1VVlbmMOXq8cXD16l9Kisr03bbbZeuvvrq9Nlnn6Uzzzwzde/ePW244YapUaNGady4cSml/3uRuNjbyQ7nqyXS/z9capNNNokZM2bEeeedF3369In+/fvHKaecEr///e8jIuKEE04o7LqsjYc5rDgxwZIlS6JZs2Zx+eWXxzPPPBMPPPBARETsvffeMWjQoGjfvn0cdthh8fLLL+c57rdSdXV17LHHHtGiRYvYY489YvHixdGpU6c47rjjYv/9949rr702Ro0aFRERTz31VBxyyCExadKkuO2222L99dePSZMmRaNGjWrtWSGB2iP9/9Ne33XXXdGrV6/4+9//HjNmzIgNN9wwTj755Dj55JNj4MCBMXr06MJtWrRoESeccEJ07tw5v8GJ559/Po4//vi46KKLCmfZXbp0aWF7Zeutt44jjzwynnnmmfjDH/4QixYtKhwCXtc88sgjceKJJ670IfLULosXL44uXbrEtddeG+utt1688sor0bdv35g8eXLsvffeceedd0ZKKRo1Wv4JTsXeTvY5UbVE69ato7y8PM4666z497//Hfvss08cdthhcfPNN8cxxxwTb7/9dkRErT/z2axZs6Jdu3aFOTt27BidOnWKf/zjH/HDH/4wIiL23HPPWLx4cdx8883RvHnzPMf9VmrQoEH84Ac/iKZNm8bgwYOjR48e8fTTTxdCKiLi6quvjpRSnHLKKXH55ZfHp59+GhFRCPilS5cWHqQAvkxJSUk899xzccwxx8Qll1wSP/nJTwqPI23bto1f/vKXERGFD209/vjjC7cjXx06dIhjjz02Lrvssrjtttvihz/8YeEFtM+foa9hw4bx85//PC677LIYM2ZMXHDBBTlPnl27du3ijTfeiMceeyz69++f9zh8zsyZM2PevHmxwQYbxPrrrx8XXXRR/OMf/4i5c+fGYYcdFmuttVZERDRr1izat2+/Zh87irqfi//ozTffTK+99lr6xz/+Ubjs1ltvTbfeemtatGhR4Qx3P/nJT9LJJ5+cli1bVisP4Vsx01tvvZXat2+fDj/88PTEE08U5r/uuutSkyZNVjrr3qeffrrGZ+X/7q+lS5emxx57LHXp0iV169ZtpUP7ttxyy3T55Zd/6e0B/pMV74MaNmxYOvDAA2ss+/wZXD/++OP085//PLVv3z7NmzfPY0wtsGjRopRSSvPmzUsXXnhh6tChQ433Y3/+/lvx/w866KA0ZMiQGu9/q81WbFOtmP/cc89Nu+yyS+HMg+TvrrvuSh07dkwbb7xxat26dfrJT35SeE/9Cv/+97/T8OHD07rrrpumTJmyRucTUTm58847U4cOHVKHDh0Kp/r+YmR88sknafjw4alVq1Zr/BcjqxtvvDEdeOCB6dFHH00777xz6tatW+rRo0d68cUX0/vvv5/69euXBg4cmBYtWlRnHmDrm1VtmCxevDg98sgjqwypIUOGpJYtW67yWHhg9arr4fD5x/XFixenlFLh8eTQQw9NBx988ErXSymlV155JS1evDh9/PHH6aOPPlpD0/JlRo0alY455pi0ww47pOuuuy5Nnz49LVy4MI0cOTJtvfXWaciQIYXrfv6+fOKJJ1LDhg3TK6+8ksfYX8sXY+nhhx9OnTp1Ss8//3xKaeXfVdasZ599Nq211lpp1KhR6V//+le67rrr0n777Zd22223wn109913p6OPPjptsskmNU6CtaaIqBw899xzqaysLF133XVp0qRJ6R//+EfabLPNUs+ePdM///nPlFJK99xzT/r+97+fNttss1x+Mb6KFU/67733XurYsWO68MILU0rL3/j30EMPpf333z9tvPHG6ZBDDkldu3ZNu+yyS/rkk09ynPjb6/NPBlOnTk0zZsxIM2fOTCkt39B59NFHU5cuXdLOO+9ceAXy1VdfTX/4wx9qvOIIrH6f//ucNWtW+vTTTwt76utSXM2cOTPNmTMnpZTSX//613TTTTellFI677zzUqtWrdL06dNTSv+3vnPnzk1Dhw5Nf//73/MZmBpOP/30tMEGG6Tzzz8/nXfeeam8vDz169cvVVVVpY8//jiNHDkybbPNNmnAgAGrvH1d2oNz9913p5KSkjRkyJD0wAMPFC4/7LDDUrdu3Tzv5WjFY95ZZ52VfvSjH9VY9re//S3tu+++6dhjj00pLX8R5o9//GN6++231/icKYmoXPz2t79NPXv2rHF43ocffpg22WST1Ldv35TS8t3j11xzTXrrrbfyHPW/Gj9+fBo6dGgaMGBAWrJkyUpnbRs7dmwaOnRoKikpSSUlJYUNd9acz2+EnX322WnbbbdNm2++edpmm23SQw89lFJa/srxo48+mrbbbru06667rnS6ck8oUByf//s888wz03bbbZc233zz1KdPn/TEE0+sdJ3aasWHqu+1117p+uuvTyUlJWns2LEppZSmTZuWevTokbp165beeeedlNLys2edccYZaeONN17lR3qwZo0fPz595zvfKRwqNXHixFRSUpJuvvnmwnXmzp2bhg8fno444ogav5N18flh+vTp6eabb0577LFH6ty5c9pnn33SE088ke6+++504IEHFv727I3Kz69//evUtWvXGh+mm1JKl19+eVpvvfUKH4uT530konJw6qmnpp122qnw/YoN1ieeeCK1bNmyzuwOr6ysTMcee2xq0aJF6tGjR+HypUuXrvRL/a9//avWB2F9d9ZZZ6X11lsvjRs3Lr300kvpgAMOSA0bNiwcrrd48eL02GOPpY022igdd9xxKaW6sfHGchMnTsx7BDL6/OPkddddl1q3bp1uvPHGdN5556XDDz88NWnSJP31r39NKdX+v8WlS5eme+65J22xxRapcePG6corr0wp/d/cDzzwQNp3331T8+bN05577pn+53/+J6277rq19kiLb5snn3wydevWLaW0/L3ZZWVl6aqrrkoppTR//vz09NNPp5RSjfes1fbfyVX5YvDNmjUrTZ48OfXq1Sv17NkztW3bNpWUlKTBgwfnNCEr3HDDDWm99dZLTz75ZI3Ln3/++bTFFlvktvfp80TUGjJ9+vQ0e/bslFJKTz31VGrSpEkaPXp0jes88cQTafPNN0/vvvtuHiN+LRMmTEg/+9nPUsOGDWt8AOvnH1y9kpO/f/zjH6l79+6FV9f++te/ppYtW6YePXqkBg0apDvvvDOltPzQvgkTJtTJVxa/zZ5//vlUUlKSRo0alfcofA1///vf089+9rN0/fXXFy778MMP06BBg1KLFi1q/WfqrXi8nzp1amrXrl3q0KFDOvDAAwvPeSt88MEH6corr0ynnnpquvDCC9O0adPyGJfPWfG5TnfddVfaeOON09ixY1N5eXkhglNaHsB9+/atsdFaFwPq6quvTscff3w6/PDD01133ZXmz59fY/kLL7yQLrnkkrTZZpulNm3apOeeey6nSb+dXnnllfT0008X9mCnlNIhhxySNtpoo/S3v/2tcKjwKaeckrbZZpta8fYQEbUG3Hvvval79+7pyiuvTAsWLEjz5s1LgwcPTptttlnhQwZXfGjYNttsk/7973/nO/CXWPGgOW/evBozvv3226lfv35piy22qHESgrr4IFtffDFc33jjjXT++een6urq9Pjjj6cNN9wwXXXVVWnu3Llp5513Tk2aNEk33nhjjdsIqbrjs88+SyNHjkyNGzcufJI7dcOTTz6ZNttss8JeqM+bPn162n333dPIkSNTSrX/MXXOnDnptddeS3feeWfaZZdd0n777VcIKS+m1T5//OMfU+fOnQvf77XXXqmkpCT99re/LVz22Wefpf333z8deuihdfo+PP3009O6666bfvWrX6UDDzwwde3aNQ0ZMmSVHw78wgsvpN122y1de+21KaXa/3dXH9x1112pffv26Xvf+15q06ZN2mGHHdIjjzySqqur04EHHpjatGmTtthii9SzZ8/UqlWrWrMHW0QV2b333puaNm2aRo0aVeO47+nTp6dTTz01NW7cOG211VZpxx13TK1bt641vxhftOJB5P7770+77rpr2nLLLdNOO+2Urr322rRgwYI0ZcqU9LOf/Sx17tw53XXXXTlP++32+Se6CRMmFM6QNW/evJRSSkcccUQaOHBg4Xo//elPU6dOndJuu+225oflG7nhhhsKe64XLVqULrroolRSUiKk6phzzjkntW7dOvXq1WulN+fvs88+6aijjspnsP9ixfPCu+++m6ZPn57efPPNlNLyx6DbbrstdevWLe2///6FV5Avv/zydMstt6SlS5faMK0FJk6cWOM5++6770677rpr2nbbbdP999+f/vjHP6Z99903bb311oX3O9fFkPrzn/+cNttss/TCCy+klJZvxzRo0CBtvfXWaeDAgYX33Kw4q2RKKQ0cOLDGiZYonueffz61bt26cHTWtGnTUklJSY29oXfeeWe67LLL0qhRowqPM7WBiCqiWbNmpR122CFdccUVKaXlGzmzZ89O99xzT+Ewhueffz6NHDky/elPf6pVvxir8tBDD6W11147jRw5Mr399tvp8MMPTy1btkyPPfZYSimlyZMnpwEDBqQNNtgg3XvvvTlP++30+Q2TM844I3Xt2jVdc801hcsrKyvT1ltvXTiT4qeffpr69OmTHn/8cRs1dUxlZWXaYIMN0vbbb184Yctnn30mpGqx/7QBes4556Stt946nXbaaYU9/Z999ln63ve+Vyvfn7Hi8eKuu+5KW2yxRerYsWMqLy9PP//5zwthf9ttt6XddtstbbXVVun4449PJSUlK32UB/mZM2dO2nPPPdPPfvazlNLyow+eeOKJdMghh6QNNtggde/ePR155JGFuKiLRydUVlam22+/PZ111lkppeVnPm7VqlW6/PLL05AhQ9I666yTBg8enCoqKlJK//d7fdxxx6VevXqtdJIlVr8//elPqU+fPiml5R+vsummmxbOvlddXb3SCctqExFVJNXV1emTTz5J3/3ud9P111+fqqqq0llnnZW6d++e1ltvvVRaWpr+9re/5T3mKq14ov/8E/6iRYvSoYcemoYPH55SWv7g27Fjx3TiiSfWuO1LL72UfvnLX9b6IKzvfvOb36TWrVunp556Kn3wwQc1lg0aNCg1a9YsDRs2LHXr1i3tuOOOhSfHuvgq47fZjBkz0jbbbJN22mknIVXLff5v689//nM64YQT0qBBg9If//jHwuVnnXVW2nTTTdNWW22VjjnmmNSnT5+01VZb1XiFvDZ56qmnUrNmzdLVV1+dnnzyyXT33XenddddNx100EHp/fffT8uWLUuPPPJIGjBgQPrRj35UZ06aVJ+tiIUVHnvssdSkSZP04IMP1rh81qxZacmSJYWoqM0bsl/m1ltvTccdd1yaNWtW+uijj9L777+ftttuu3TJJZeklJafkr1Nmzapffv26eKLL04pLf87ff/999Pmm29e69+LWNetCNSBAwemww8/PC1dujS1a9cuDRgwoPB7N2bMmHTZZZcVblPbXuwVUUUwevToNGrUqPTJJ5+kI444Iu2www6pRYsW6cADD0yjRo1Ks2bNSt///vcLpV2brHiif+edd9If//jHGocX9uzZMz377LNp9uzZqU2bNjU+K+Luu+8uvMJo93e+Zs6cmXbeeed0++2317h8xX377rvvppNPPjn16NEj/fSnP63TrzKy/P5ecXjtipByaF/tNWTIkLTeeuulww8/PO23336pcePGqV+/foXl5513XlpnnXXS97///cLZ0VKqnRuxw4cPT/vtt1+Ny1566aXUqlWrdPLJJ9e4vL48L6zYiJszZ06tff/yl7nkkktSr1690qWXXppSWr4u1dXVqW/fvukXv/hFWrhw4SoP26ttG65f1bnnnpt22GGH9Nprr6WUlr//sGPHjoXvX3zxxfTjH/84/fGPf1zpBcQvnlab1Wv06NHp8ssvTyktv1823XTT1Lx58/SLX/yixvVOOumkdPjhh9fa+0NErWazZs1K2267bTr//PNTSsvPNnLnnXem6667rsaZYHr37p3OPvvsvMZcpRUPIi+//HLaYost0kEHHVTj1anvf//7qXfv3mnTTTdNP//5zwsb3xUVFalPnz7piiuuqLMPtnXZFx/8X3/99dSiRYs0bty4la67ePHiQix9/hXJ2riBxlc3c+bM1KlTp9S1a9eVQqpJkybpoosuynnCb6/P/30+99xzacMNNyycLnrJkiXp0UcfTS1btkzHH3984XojRoxI3bt3T8OHDy+8l7G2PbZWV1enY445Ju2zzz4ppeXrueL9lzfffHNaf/3104wZMwrrX9vm/ybuvvvu1K1bt7TJJpuk0047rda+l/mLnnvuuXT88cenLbfcMm2//fbp2muvTZ988km644470nrrrVd4P15dv69WvAcvpZR23nnn9P3vfz+ltDyattxyy3ThhRemKVOmpP333z/179+/sL6ffyGxrv83qM1WbCdfcMEFKaXlz19HHXVU6tixY+HkOh9++GEaPnx4Wm+99dK//vWvPMf9j0TUarLiieKJJ55IXbt2/dJPYJ89e3bhF+P1119fkyN+JVOmTEmtWrVKQ4cOTe+//36NZQ899FDadNNNU6dOnWpcfsYZZ6TNNtvM50Dl4PMbaHfffXeaPn16YYP6T3/600qf5/Hggw+m888/35NFHbbi/nr99dfTxIkT0zPPPJNSWv5EtPXWW68UUr/5zW/SOuusU/hgQtacz/9tLVmyJP31r39NHTt2XOlV1TvuuCO1atWqEFcpLT+0r2vXrmngwIHp448/XmMzf5nP74H59NNPU0rL319SWlpaeF/siseje+65J3Xu3LnGxmxd9vn7ceLEiWm99dZLv/71r9P555+fNtlkk3TQQQcVPj6iNrrhhhvS4MGD069//ev0l7/8Jc2aNSuddNJJqVu3bqlDhw5p7Nixaf3110/9+vWr80cknHfeeWnfffdN999/f0pp+Zlpt9hii/S73/0uLV26NJ144olp0003TW3atEnf+973Ci8Gex4svi9uJ48fP76wbMKECemoo45KrVq1Sptuumnq2rVr6tChQ61/gUJErWY777xz+ulPf7rKZXfffXc6+uij08Ybb1wrfzEWLlyYDjnkkJV2py5evDh9+OGH6e9//3s677zz0jbbbJO+//3vp0GDBqW+ffumli1b1sr1qe8+/6A/bNiw1LZt28KhW/369Uvrr79+eu655wrXW7hwYTrggANqvPJG3bLifrvnnntShw4dUufOnVOzZs1S//7906xZs9KMGTMKIbXiVeUVJ7RhzXriiSfSLbfcklJK6fjjj0+nnHJKeumll9Laa6+dHnnkkRrXnTp1atpwww3TAw88UOPywYMHpx49etSKiEpp+e9d9+7d0+abb57OOuus9NBDD6VBgwalzp07p0cffbRwvaFDh6Ydd9yxzof7bbfdlqZMmVL4/s0330wXX3xxOvfccwuXTZw4Me24446pd+/eK30oaG0wZMiQtMEGG6RTTjklHXLIIaljx46FkyxMmzYtnXHGGWnLLbdMJSUl6cADD6zTzw1Lly5Nhx56aCopKUlrr712GjZsWHrxxRfT8OHD06GHHppmzJiRFi5cmP75z3+mZ555phCMdflIjLoYvV+2nfzxxx+n//3f/00XX3xx+utf/1onPjNVRK0GKx50HnroobTrrrvWOPvQvHnz0tSpU9N9992XJk6cmK6++upau8dm8eLFabfddiucTTCllB5++OF08sknp7KysrTNNtsUzt3/k5/8JB1wwAFp0KBBNZ5kWPPOOeectO6666YJEyYUPnyuuro6HXrooWn99ddPAwYMSIMGDUq777572nrrrb3yVsc98sgjqWXLlunaa69NVVVV6cEHH0wlJSXpsMMOSzNnzkwzZsxI2223Xdpss81W2ptcl9WlDZ3Kysq09957px49eqQDDjggtWjRIk2ePDlVVFSkH/3oR6lPnz41jlaYPXt22mabbdJ9992XUqq5rrXlfTcvvPBCKi8vT+ecc04aNGhQ2nHHHVPfvn3T7373u3TyySenxo0bp5133jl17969XrywNnPmzLTbbrsVPppk7ty5qW3btqlZs2bpl7/8ZY3r/u///m/aYYcd0iGHHLJSIOfpkUceSZtuumn63//935RSSrfffntq2rRp4VTSK/zzn/9Mt956a+H3ri4/NzzxxBOpX79+6aqrrkp77rlnOuGEE9Khhx6aOnToUHgPzufVxQhZ4a233koXX3xx4ciD2uw/bSfPnTs3TZ06Nd166615jfe1iajV6KijjkoHHnhgWrx4caqurk5/+9vfUu/evdOWW26Zdt9997R48eJavSFQUVGRttxyy3TcccelKVOmpAsuuCB16tQpHXzwwWnUqFHpuuuuS1tuuWU688wzC7epyw+29cGcOXPSXnvtVXjF+7333ktPPvlkGjBgQLrjjjvS0UcfnY488sjUq1evdPLJJxd+/2rz7yFfrqKiIg0YMKDwfsq33347bbbZZumQQw5J5eXl6Uc/+lGaPn16mj59etpll13S22+/nfPEq8fQoUNXOlFKbTdnzpzUqVOnVFJSUvhIgZRSuu+++1LPnj1Tjx490uWXX57uu+++tPfee6cddtihxgZdbTpT5ptvvpnOPffcdN555xUuu//++9Nee+2VfvzjH6f77rsvPf3002no0KHpoosuSlOnTs1x2tVn4cKFKaXl7xOeO3duev7559PGG2+cdtttt/TSSy/VuO7EiRNTx44d0xFHHFE43DFvN9xwQ+rZs2dKafkho82bN09XX311Smn5iRNWteesLj43/O53v0u/+93vUkrL/26OPvro1L9//7Ro0aJ00003pWOPPTaVlJSkkpKS9PLLL+c87erz/PPPp2bNmqXzzjuvzrxg9p+2k3v06JEqKyvr1HaliFpNnnrqqdSmTZv0xhtvpLFjx6ajjz46rbXWWmnQoEGFVxfrgr/97W+pUaNGaZNNNknNmzdP11xzTeEzrRYvXpz22WefGrth69Ive300d+7ctNFGG6UzzjgjPf300+mwww5L3/ve99KOO+6Y2rZtWzi71+efGOvikyTLVVVVpTvuuCO9+eabac6cOWn77bcvfMbLX/7yl1RSUpJ+8IMfpPfee6/e3M+nnnpq6tq1a63ZI/NVffLJJ2m//fZLu+++e9p7773TTTfdVFj20EMPpZNOOimVl5ennXbaKfXq1avWniWzoqIide3aNa2//vpp6NChNZbdf//9aY899kh9+vRZKSrqi4qKivTd7343HX744WnOnDnp+eefT+3bt0/9+/dfaYP8hRdeqBUvXKx4Xr7mmmtS375904MPPpjKysoKAZXS8pg//fTT00cffZTXmKtFVVVVOvfcc1PDhg1T37590+OPP56WLl2atttuuzRy5MiU0vK/qVNPPTXtvffete7v65t67rnnUrt27dI555xT60Oqvmwnf56IWk3OPvvstM4666SuXbumtm3bpl//+tfp2WefrXGduhIcM2bMSJMmTVppo2XZsmXpxz/+cWFPVF1Zn/ruuuuuS61atUotWrRIv/rVrwpv8j7iiCNqnDo5JfdZfbDiszXGjBmTdtlll8KhHLfeemvq2bNn2mSTTerEseRfxSOPPJL22GOPWvOeoK/jgw8+SPvtt1/aY489aoRUSsvPUvXJJ5/U+s/iefHFF9MWW2yRunfvvtKH5T7wwANpu+22K+yBqY+PMRMnTkxdu3ZNxxxzTJo7d2567rnnCiFVmz/76rXXXkuNGzdOJSUl6YYbbihcvnDhwtSrV690zDHH1Jv769VXX00HHXRQ6tatWzrmmGPSLbfckvr06ZMmTpxYuM6qzsJXHzz77LOpXbt26fzzz6/Vj5X1aTt5BRG1GixZsiQde+yxabfddktDhw6t8aRY134hvkxVVVU688wz00YbbVRvDtWoT959990a98uyZcvSnnvumc4444wcp6KYVpzkZcWb94cOHZquuOKKWvvBrF/H/PnzC4dU1WVvv/12+uEPf5j23nvv9Oc//zktXbo07b777mnYsGGF69Smw/dW5Z///Gfabrvt0oABA1YKqUceeSRNnz49p8nWjBdffDFtt912NUJq0003TQcffHDhc4dqo5tuuik1a9Ys/epXv0pPPvlkeuKJJ9Lee++dtt1223rxHqjP+/e//53uueeetNNOO6XS0tLUunXrNGLEiBrXqS/r+kXPPfdc2mabbdIll1xSKz9Tqb5uJ5eklFLwjVVUVERKKVq2bBkREdXV1dGgQYN8h1pNbrnllpg4cWKMHTs2Hnroodh+++3zHqlWSSlFSUlJ3mNERMSCBQti8uTJcdFFF8W7774bL774YjRq1CjvsWql2nS/fR2TJ0+Obt26RdeuXaNp06YxceLEePbZZ2PbbbfNezRW4Z133onBgwfHlClToqqqKtZaa6144YUXokmTJnmP9pW99NJLceyxx8YOO+wQp5xySmy11VZ5j7RGvfTSS3HMMcfEDjvsEJdeemlMnjw5fvnLX8YjjzwSG220Ud7jrdLSpUvjzjvvjMGDB0dExIYbbhgbbbRR3HXXXdG4ceNYtmxZNGzYMOcpV79f//rXMWrUqOjatWs8+eSTeY+zRjz77LNx//33x8UXXxwLFiyIsrKyvEeqoT5uJ4uoIqjrG2ef98Ybb8QJJ5wQrVq1ivPPPz86d+6c90i1zkcffRQbbLBB3mNESimefvrpuPTSS2PJkiXx17/+tV4/SX5TteV++yaef/75uOqqq6K8vDx+/vOfx9Zbb533SPwHH3zwQbzwwgvx0UcfxVFHHRWNGjWKpUuX1qkXOl566aU44YQTYtNNN43f/OY3seWWW+Y90hr10ksvxYABA2LTTTeNP/7xj9GkSZNo1qxZ3mP9V//+979j3rx50bRp02jXrl2UlJTUud+9r+Lz21+TJk2K7bffPho2bFivtsv+m1GjRsWHH34YJ598cmy44YZ5j7NK9eX+EFH8Vx9//HGUlpZGeXl53qPUOu+++25sttlmMXr06PjpT3+a9zhRVVUV//rXv6JLly7RoEGDevkkuTrUtvvtm6iuro6SkpJ68YT0bVNXX+CYOHFiDBkyJG699dZo06ZN3uOscRMnTozBgwfHbbfdVmfXvz7sBfgyX9xAr6t/Z1/XE088EQMHDoynnnoq1l133bzHqddEFHwD8+fPj1NPPTXKysrisssuy3ucGurzk+Q3VZvvN6gLFi1aFE2bNs17jNx829ef2m3hwoWx1lpr5T1GvWcLC76B5s2bx8knnxxTp06NJUuW5D1ODQLqy9Xm+w3qgm97QHzb15/aTUCtGfZEwWrgVZ+6yf0GAHwdIgoAACADx/sAAABkIKIAAAAyEFE5q6qqihEjRkRVVVXeo6x21q1usm51k3Wrm6xb3WTd6ibrVjfV1nXznqicVVZWRnl5eVRUVESLFi3yHme1sm51k3Wrm6xb3WTd6ibrVjdZt7qptq6bPVEAAAAZiCgAAIAMGuU9QG1UXV0ds2bNiubNm0dJSUlR/63Kysoa/1ufWLe6ybrVTdatbrJudZN1q5usW920ptctpRTz58+PjTbaKBo0+PL9Td4TtQrvvfdetG/fPu8xAACAHMycOTPatWv3pcvtiVqF5s2b5z1CUVVUVOQ9QtG0br1e3iMUTdOma+c9QtEsWPBJ3iMAABT8tx4QUatQ7EP48labzmyyutXn+64+rxsAQG3y37a7nFgCAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGRQ9oj755JNYsGBBUf+NRYsWxb///e+i/hsAAAARRYqopUuXxgMPPBCHHnpotGnTJt56661YvHhxnHTSSdGmTZto2rRpdOjQIUaOHFm4zYwZM+LAAw+MsrKyaNGiRRx66KHx0UcfFZb/85//jD322COaN28eLVq0iB133DEmTZoUEREfffRRtG3bNnr37h333HNPLF68uBirBQAAsHoj6pVXXonBgwdHu3btol+/ftG6det48skno0uXLvH73/8+7r///rj99tvjjTfeiFtuuSU6dOgQEREppejdu3fMnTs3nn766XjsscfirbfeisMOO6zws4844oho165dTJw4MV544YUYOnRoNG7cOCIiNtlkk3j++edjk002ieOPPz422mijGDhwYLzwwgtfae6qqqqorKys8QUAALAqJSml9E1+wJw5c2LMmDExevToeO211+IHP/hB9OvXL/bff/9o0qRJ4XoDBw6M1157LR5//PEoKSmp8TMee+yx+MEPfhDvvPNOtG/fPiIi/vWvf8XWW28dEyZMiJ122ilatGgRV1xxRRx11FH/cZ6lS5fGQw89FDfddFP89a9/je985ztx1FFHxZFHHhkbbLDBKm8zYsSIOPvss7/Jf4Y65Rve5bVakyZN8x6haJo2XTvvEYpm/vy5eY8AAFBQUVERLVq0+NLl33hP1BVXXBGDBg2KsrKyePPNN+Pee++NPn361AioiIj+/fvH5MmTo1OnTjFw4MB49NFHC8umTJkS7du3LwRURMRWW20VLVu2jClTpkRExKmnnhrHHnts7LXXXnHhhRfGW2+9tcp5GjVqFAcccEDccccdMX369GjTpk0MGTKkxqGDXzRs2LCoqKgofM2cOfOb/CcBAADqsW8cUQMGDIjzzjsvPvzww9hqq62if//+8be//S2qq6trXG+HHXaId955J84999z47LPP4tBDD41DDjkkIpbvGfni3qkvXj5ixIh47bXX4oc//GE88cQTsdVWW8U999yzyts888wzcdxxx8WWW24Z06ZNi7POOitOPfXUL12H0tLSaNGiRY0vAACAVfnGh/N93vjx4+PGG2+MsWPHRvPmzeOII46II488MrbeeuuVrvvII49Er169Ys6cOfHCCy986eF8EydOjK5du650+8MPPzw+/fTTuP/++yMiYurUqXHzzTfHLbfcErNnz45DDjkkjjrqqOjRo8cqA+0/qaysjPLy8q/xX6BucDhf3eRwPgCANeO/Hc63WiNqhUWLFsW9994bN954Yzz22GPx0ksvxeOPPx5t2rSJ7bbbLho0aBC//e1v44EHHoj3338/SkpKYscdd4yysrIYNWpULF26NE488cQoKyuLp556Kj777LMYMmRIHHLIIdGxY8d477334qijjoqDDz44LrroopgxY0Z07NgxevbsWbh87bW//ganiKq7RFTdJKIAgNrkv0VUo2L8o02bNo2+fftG3759Y9asWVFWVhZlZWVx0UUXxbRp06Jhw4ax0047xYMPPhgNGiw/ovDee++NX/7yl7H77rtHgwYNolevXnHFFVdERETDhg1jzpw50a9fv/joo49i3XXXjT59+hROBrHuuuvGO++8ExtvvHExVgcAAKCgKHui6jp7ououe6LqJnuiAIDapOhn5wMAAPg2EVEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABo3yHqD2K8l7gNVu8822z3uEotm4fee8RyiaYVddkvcIRTNgv33zHqFoSkrq32PICsuWLct7hKKpz/dbSinvEfha3G/UNvX1cfKr/a3ZEwUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGjfIe4IvGjx8fJ5544iqX9erVKyZNmhSzZ89e5fIJEybENddcE9dff/0ql5955plxyCGHrLZZAQCAb59aF1GVlZXRu3fvGDFiRI3Lp0+fHkOHDo0FCxbE5MmTV7pdz549o7q6OmbNmhWjRo2Knj171lg+evToL40vAACAr8rhfAAAABnUuj1ReaiqqoqqqqrC95WVlTlOAwAA1Gb2REXEyJEjo7y8vPDVvn37vEcCAABqKREVEcOGDYuKiorC18yZM/MeCQAAqKUczhcRpaWlUVpamvcYAABAHWBPFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZFDrzs5XXl4e48aNi3Hjxq20bN9994158+ZF165dV3nbBg0aRLt27WLw4MGrXD58+PDVOisAAPDtU5JSSnkPUdtUVlZGeXn5//+uJNdZimGzTbvkPQJfw7CrLsl7hKIZsN++eY9QNCUl9e8xZIVly5blPULR1Of7zdN+XeV+o7apr4+Ty//WKioqokWLFl96LYfzAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGRQklJKeQ9R21RWVkZ5eXk0atQkSkpK8h6HDNZeu2XeIxRNff5d3GWXA/MeoWg++ODtvEcomhkzXst7hKJp3rx13iMUzQcfvJX3CEWzePGivEfga6jfm6L1d90aNy7Ne4SiSCnF0qWLo6KiIlq0aPGl17MnCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAwarc4fNn78+DjxxBNXuaxXr14xadKkmD179iqXT5gwIa655pq4/vrrV7n8zDPPjK5du0bv3r1XuXzbbbeNm266Kfr16xcvv/zyKq9z7733RocOHf7regAAAHyZ1RpRlZWV0bt37xgxYkSNy6dPnx5Dhw6NBQsWxOTJk1e6Xc+ePaO6ujpmzZoVo0aNip49e9ZYPnr06Jg9e3YsWrQotttuuxg9evRKP6Nbt24RETF16tRV/hv9+/ePRYsWfc01AwAAWM7hfAAAABms1j1RdVVVVVVUVVUVvq+srMxxGgAAoDazJyoiRo4cGeXl5YWv9u3b5z0SAABQS4moiBg2bFhUVFQUvmbOnJn3SAAAQC3lcL6IKC0tjdLS0rzHAAAA6gB7ogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIIPVena+8vLyGDduXIwbN26lZfvuu2/MmzcvunbtusrbNmjQINq1axeDBw9e5fLhw4dHs2bN4tVXX13lz/jud78bERGdO3f+0n+jWbNmX3VVAAAAVqkkpZTyHqK2qaysjPLy8mjUqEmUlJTkPQ4ZrL12y7xHKJr6/Lu4yy4H5j1C0Xzwwdt5j1A0M2a8lvcIRdO8eeu8RyiaDz54K+8Rimbx4kV5j8DXUL83RevvujVuXD8/HiilFEuXLo6Kiopo0aLFl17P4XwAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGTTKe4DarFnTtaOkpP51ZuX8OXmPUDSfLazMe4SiqVq8KO8Riubll5/Oe4SiadF8nbxHKJply5blPULRfLpgXt4jFE27dp3yHqFoKir+nfcIRVNdXX//3qqqFuY9QtEsXbok7xGKprR0rbxHKIqUqmP+/Ln/9Xr1rxAAAACKSEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABk0CjvAWqDqqqqqKqqKnxfWVmZ4zQAAEBtZk9URIwcOTLKy8sLX+3bt897JAAAoJYSURExbNiwqKioKHzNnDkz75EAAIBayuF8EVFaWhqlpaV5jwEAANQB9kQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIoFHeA9RmVYsXRUlJSd5jkMGy6mV5j8DXUJ//zlqv2zbvEYpm5nuv5z1C0ZSWNst7hKJp0qRp3iPwNSxdsjjvEYpm6dIleY9QPCnlPUHRVFUtzHuEokhf8T6zJwoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJBBo7wHqA2qqqqiqqqq8H1lZWWO0wAAALWZPVERMXLkyCgvLy98tW/fPu+RAACAWkpERcSwYcOioqKi8DVz5sy8RwIAAGoph/NFRGlpaZSWluY9BgAAUAfYEwUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACCDkpRSynuI2qaysjLKy8sjoiRKSkryHme1S6k67xH4GkqbNMt7hKJZvKQq7xGKZttte+Q9QtHssf+P8h6haG6+5uK8RyiaTz+tzHuEoknVy/IeoWgaN2ma9whF07Bho7xHKJr58+fmPULR1NeEWL5eKSoqKqJFixZfej17ogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABk0ynuA2qCqqiqqqqoK31dWVuY4DQAAUJvV+z1RY8aMibKyssLXs88+u9J1Ro4cGeXl5YWv9u3b5zApAABQF9T7PVE/+tGPYueddy5837Zt25WuM2zYsDj11FML31dWVgopAABglep9RDVv3jyaN2/+H69TWloapaWla2giAACgLqv3h/MBAACsTiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEpSSinvIWqbysrKKC8vjyaNm0ZJSUne46x2VYsX5T1CEdXfX+fSJs3yHqFolixdnPcIRdO8+Tp5j1A0qbo67xGKpkPH7+Y9QtHMnTMr7xGKpqwe/73Nen9a3iMUTcdNt817hKKZMuX5vEcompKS+rkvJqUUixd/FhUVFdGiRYsvvV79XHsAAIAiEVEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZNMp7gNqgqqoqqqqqCt9XVlbmOA0AAFCb2RMVESNHjozy8vLCV/v27fMeCQAAqKVEVEQMGzYsKioqCl8zZ87MeyQAAKCWcjhfRJSWlkZpaWneYwAAAHWAPVEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyaJT3ALXZ0mVLoqSkJO8xVrv6uE4rpJTyHqFoGjZqnPcIRbN02ZK8Ryia9dffJO8Riqa6elneIxTN0iWL8x6haOZ+8mHeIxTN4iVVeY9QNK3XbZv3CEUzd279/Z2srq7Oe4SiSWlp3iMUxVfdlrQnCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAwaZbny+PHj48QTT1zlsl69esWkSZNi9uzZq1w+YcKEuOaaa+L6669f5fIzzzwzunbtGr17917l8m233TZuuumm6NevX7z88survM69994bkyZNivPOO2+Vy4855pgYOHDgKpcBAAB8FZkiqrKyMnr37h0jRoyocfn06dNj6NChsWDBgpg8efJKt+vZs2dUV1fHrFmzYtSoUdGzZ88ay0ePHh2zZ8+ORYsWxXbbbRejR49e6Wd069YtIiKmTp26yn+jf//+sWjRopg9e3acfPLJ0b9//xrLn3rqqXj44YczrC0AAMDKHM4HAACQQaY9UfVVVVVVVFVVFb6vrKzMcRoAAKA2sycqIkaOHBnl5eWFr/bt2+c9EgAAUEuJqIgYNmxYVFRUFL5mzpyZ90gAAEAt5XC+iCgtLY3S0tK8xwAAAOoAe6IAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACCDTGfnKy8vj3HjxsW4ceNWWrbvvvvGvHnzomvXrqu8bYMGDaJdu3YxePDgVS4fPnx4NGvWLF599dVV/ozvfve7ERHRuXPnL/03mjVrFuuvv35ccMEF8Yc//GGl5f379/+yVQMAAPhKSlJKKe8hapvKysooLy+PBg0aRklJSd7jrHbV1dV5j1A0KdXfdVtrrRZ5j1A0VVUL8x6haDbddLu8Ryia6upleY9QNKVNmuU9QtFMf/fVvEcomrKyVnmPUDRrr12e9whFs3TpkrxHKJoPPngr7xGKpr5uc6WUorp6WVRUVESLFl++7eVwPgAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMGuU9QG229trlUVJS/zqzsnJ23iPwNSxcOD/vEYqmWbOyvEcomiZNmuY9QtFMn/5K3iMUTfv2nfMeoWiWLl2c9whFU1JSkvcIRVNaulbeIxTNvHnT8x6haBo3Ls17hKJZa63meY9QFNXV1fHJJx/+1+vVv0IAAAAoIhEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADBrlPUBW48ePjxNPPHGVy3r16hWTJk2K2bNnr3L5hAkTokmTJsUcDwAAqOfqXERVVlZG7969Y8SIETUunz59egwdOjQWLFgQkydPXul2PXv2jOrq6jUzJAAAUG85nA8AACCDOrcnqhiqqqqiqqqq8H1lZWWO0wAAALWZPVERMXLkyCgvLy98tW/fPu+RAACAWkpERcSwYcOioqKi8DVz5sy8RwIAAGoph/NFRGlpaZSWluY9BgAAUAfYEwUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABnUubPzlZeXx7hx42LcuHErLdt3331j3rx50bVr11XetkEDzQgAAHwzdS6idtlll5g0aVLeYwAAAN9Sds0AAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZNAo7wFqs/nz50ZESd5jwP+X8h6gaD77bH7eIxTN1KkT8x6haKqrl+U9QtE0bFh/nx6XLFmc9whFM++Tj/IeoWiOOOHUvEcomifuuzfvEYrmrbcm5z1C0cyZ80HeIxTJV9vesicKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADBrlPcAXjR8/Pk488cRVLuvVq1dMmjQpZs+evcrlEyZMiGuuuSauv/76VS4/88wz45BDDlltswIAAN8+tS6iKisro3fv3jFixIgal0+fPj2GDh0aCxYsiMmTJ690u549e0Z1dXXMmjUrRo0aFT179qyxfPTo0V8aXwAAAF+Vw/kAAAAyqHV7ovJQVVUVVVVVhe8rKytznAYAAKjN7ImKiJEjR0Z5eXnhq3379nmPBAAA1FIiKiKGDRsWFRUVha+ZM2fmPRIAAFBLOZwvIkpLS6O0tDTvMQAAgDrAnigAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMig1p2dr7y8PMaNGxfjxo1badm+++4b8+bNi65du67ytg0aNIh27drF4MGDV7l8+PDhq3VWAADg26fWRdQuu+wSkyZN+tq3P+mkk+Kkk05ajRMBAAD8H4fzAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGTQKO8BAOqz5s3XyXuEopk/f27eIxTNJptsnfcIRfPOOy/nPULRNGrYOO8Riubmqy7Je4Si6fn9w/IeoWim/Ov5vEegSOyJAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIINGa/IfGz9+fJx44omrXNarV6+YNGlSzJ49e5XLJ0yYENdcc01cf/31q1x+5plnRteuXaN3796rXL7tttvGTTfd9LXmBgAAWGGNRlRlZWX07t07RowYUePy6dOnx9ChQ2PBggUxefLklW7Xs2fPqK6ujlmzZsWoUaOiZ8+eNZaPHj06Zs+eHYsWLYrtttsuRo8evdLP6Nat2+pbEQAA4FvL4XwAAAAZrNE9UbVVVVVVVFVVFb6vrKzMcRoAAKA2sycqIkaOHBnl5eWFr/bt2+c9EgAAUEuJqIgYNmxYVFRUFL5mzpyZ90gAAEAt5XC+iCgtLY3S0tK8xwAAAOoAe6IAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACCDNXp2vvLy8hg3blyMGzdupWX77rtvzJs3L7p27brK2zZo0CDatWsXgwcPXuXy4cOHR7NmzeLVV19d5c/47ne/+82GBwAAiDUcUbvssktMmjTpa9/+pJNOipNOOuk/Xueb/HwAAID/xuF8AAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABk0ynuA2qxBg4ZRUlKS9xir3bJlS/MeAb41Fiz4JO8RimbJksV5j1A0L774WN4jFE19fg5o1LBx3iMUzWFHD8x7hKK5Z8y1eY9QNJtutl3eIxTNtGkv5D1CUaSUYtmyJf/1evZEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkMG3IqI6dOgQo0aNynsMAACgHvhWRBQAAMDqIqIAAAAyaJT3ALVBVVVVVFVVFb6vrKzMcRoAAKA2+1bsiWrQoEE0aPDlqzpy5MgoLy8vfLVv334NTgcAANQl34qIKisri7Kysi9dPmzYsKioqCh8zZw5cw1OBwAA1CXfisP5ysvL/2NElZaWRmlp6RqcCAAAqKu+FRH17LPP5j0CAABQT3wrDufbc88946abbsp7DAAAoB74VkTUW2+9FXPnzs17DAAAoB74VhzON3369LxHAAAA6olvxZ4oAACA1UVEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABk0ynuA2mzttVpESUn968zK+XPyHgG+NZYtW5r3CHwNCxZ8kvcIfA2t122b9whF89yjD+Y9QtGUlq6V9whF07btd/IeoWhmzXoz7xGKIqXqqKz879vK9a8QAAAAikhEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkEHRIuqTTz6JBQsWFOvH1zBjxow18u8AAACs1ohaunRpPPDAA3HooYdGmzZt4q233oqIiPfffz8OO+ywaNWqVbRu3ToOPPDAmD59euF21dXVcc4550S7du2itLQ0tttuu3j44YcLyxcvXhwnnXRStGnTJpo2bRodOnSIkSNHFpYfddRRsc0228TFF18cH3zwwepcJQAAgBpWS0S98sorMXjw4GjXrl3069cvWrduHU8++WR06dIlFi5cGHvssUeUlZXFM888E88991yUlZVFr169YvHixRERcfnll8ell14al1xySbz88sux7777xo9+9KOYNm1aRET8/ve/j/vvvz9uv/32eOONN+KWW26JDh06FP7922+/PQYMGBBjx46N9u3bx3777Rdjx46NRYsWfaX5q6qqorKyssYXAADAqpSklNLXueGcOXNizJgxMXr06HjttdfiBz/4QfTr1y/233//aNKkSeF6119/ffz2t7+NKVOmRElJSUQs37PUsmXLuPfee2OfffaJtm3bxi9+8YsYPnx44Xbf+973Yqeddoorr7wyBg4cGK+99lo8/vjjhZ/xZaZMmRI33nhjjBkzJhYsWBCHHXZY9O/fP7p16/altxkxYkScffbZK13evKxVlJTUv7eNVc6fk/cI8K3RsGGjvEcommXLluU9QtGUlbXMe4SiWbKkKu8RimajjTbPe4SiKS9fN+8Riqaysv5ul2y6aZe8RyiaiRMfynuEokipOior50RFRUW0aNHiS6/3tQvhiiuuiEGDBkVZWVm8+eabce+990afPn1qBFRExAsvvBBvvvlmNG/ePMrKyqKsrCzWWWedWLRoUbz11ltRWVkZs2bNiu7du9e4Xffu3WPKlCkREdG/f/+YPHlydOrUKQYOHBiPPvrol87VuXPnuPDCC+Pdd9+NYcOGxfXXXx+9evX6j+sybNiwqKioKHzNnDnza/5XAQAA6ruv/RLpgAEDonHjxnHjjTfGVlttFQcffHAceeSRsccee0SDBv/XZtXV1bHjjjvGmDFjVvoZ6623XuH/f3EPU0qpcNkOO+wQ77zzTjz00EPx+OOPx6GHHhp77bVX3HnnnSv9zJkzZ8aYMWPi5ptvjnfeeSd+/OMfx9FHH/0f16W0tDRKS0szrT8AAPDt9LX3RG200UZxxhlnxNSpU+ORRx6J0tLSOPjgg2OTTTaJoUOHxmuvvRYRywNo2rRpsf7668fmm29e46u8vDxatGgRG220UTz33HM1fv748eOjc+fOhe9btGgRhx12WPzpT3+KsWPHxl133RVz586NiIj58+fH6NGjY88994wOHTrEAw88EKecckp8+OGHMWbMmNhrr72+7moCAADUsFre8LPrrrvGtddeGx9++GFcfPHF8c9//jO6dOkSr7zyShxxxBGx7rrrxoEHHhjPPvtsvPPOO/H000/HoEGD4r333ouIiCFDhsRFF10UY8eOjTfeeCOGDh0akydPjkGDBkVExGWXXRa33XZbvP766zF16tS44447YsMNN4yWLVtGRETv3r3j7LPPju7du8fUqVPj2WefjWOPPfY/HscIAADwdazWdzw3bdo0+vbtG3379o1Zs2ZFWVlZrLXWWvHMM8/E6aefHn369In58+dH27ZtY8899yxEzsCBA6OysjJOO+20+Pjjj2OrrbaK+++/P77zne9ERERZWVlcdNFFMW3atGjYsGHstNNO8eCDDxYOG7zqqqtiiy22+K8nnQAAAPimvvbZ+eqzysrKKC8vd3Y+4Btzdr66ydn56iZn56ubnJ2vbnJ2PgAAAL4yEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABk0CjvAWqzJk3WigYN6mFnzp+T9wTwBSV5D1A0DRo0zHuEolm2bFneIxTN0qVL8h6haEpK6uHz2v+3cGFl3iMUTatWG+Y9QtHU79/J+XmPUDTNmjXPe4SiqK6ujsrK/76tXH9/awEAAIpARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADEQUAABABiIKAAAgAxEFAACQgYgCAADIQEQBAABkIKIAAAAyEFEAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJBB0SLqk08+iQULFhTrx9cwY8aMNfLvAAAArNaIWrp0aTzwwANx6KGHRps2beKtt96KiIj3338/DjvssGjVqlW0bt06DjzwwJg+fXrhdtXV1XHOOedEu3btorS0NLbbbrt4+OGHC8sXL14cJ510UrRp0yaaNm0aHTp0iJEjRxaWH3XUUbHNNtvExRdfHB988MHqXCUAAIAaVktEvfLKKzF48OBo165d9OvXL1q3bh1PPvlkdOnSJRYuXBh77LFHlJWVxTPPPBPPPfdclJWVRa9evWLx4sUREXH55ZfHpZdeGpdcckm8/PLLse+++8aPfvSjmDZtWkRE/P73v4/7778/br/99njjjTfilltuiQ4dOhT+/dtvvz0GDBgQY8eOjfbt28d+++0XY8eOjUWLFn2l+auqqqKysrLGFwAAwKqUpJTS17nhnDlzYsyYMTF69Oh47bXX4gc/+EH069cv9t9//2jSpEnhetdff3389re/jSlTpkRJSUlELN+z1LJly7j33ntjn332ibZt28YvfvGLGD58eOF23/ve92KnnXaKK6+8MgYOHBivvfZaPP7444Wf8WWmTJkSN954Y4wZMyYWLFgQhx12WPTv3z+6dev2pbcZMWJEnH322Std3nqdttGgQf1729i/Z8/MewT4gv/8d12XNW7c5L9fqY5asmRx3iMUTdOma+c9Al9Defm6eY9QNG3bbpH3CEVTUfHvvEcomg026JD3CEXz9tv/zHuEoqiuro6PP54eFRUV0aJFiy+93tcuhCuuuCIGDRoUZWVl8eabb8a9994bffr0qRFQEREvvPBCvPnmm9G8efMoKyuLsrKyWGeddWLRokXx1ltvRWVlZcyaNSu6d+9e43bdu3ePKVOmRERE//79Y/LkydGpU6cYOHBgPProo186V+fOnePCCy+Md999N4YNGxbXX3999OrV6z+uy7Bhw6KioqLwNXOmyAAAAFat0de94YABA6Jx48Zx4403xlZbbRUHH3xwHHnkkbHHHnvU2HtTXV0dO+64Y4wZM2aln7HeeusV/v8X9zCllAqX7bDDDvHOO+/EQw89FI8//ngceuihsddee8Wdd9650s+cOXNmjBkzJm6++eZ455134sc//nEcffTR/3FdSktLo7S0NNP6AwAA305fe0/URhttFGeccUZMnTo1HnnkkSgtLY2DDz44Ntlkkxg6dGi89tprEbE8gKZNmxbrr79+bL755jW+ysvLo0WLFrHRRhvFc889V+Pnjx8/Pjp37lz4vkWLFnHYYYfFn/70pxg7dmzcddddMXfu3IiImD9/fowePTr23HPP6NChQzzwwANxyimnxIcffhhjxoyJvfba6+uuJgAAQA2r5Q0/u+66a1x77bXx4YcfxsUXXxz//Oc/o0uXLvHKK6/EEUccEeuuu24ceOCB8eyzz8Y777wTTz/9dAwaNCjee++9iIgYMmRIXHTRRTF27Nh44403YujQoTF58uQYNGhQRERcdtllcdttt8Xrr78eU6dOjTvuuCM23HDDaNmyZURE9O7dO84+++zo3r17TJ06NZ599tk49thj/+NxjAAAAF/H1z6cb1WaNm0affv2jb59+8asWbOirKws1lprrXjmmWfi9NNPjz59+sT8+fOjbdu2seeeexYiZ+DAgVFZWRmnnXZafPzxx7HVVlvF/fffH9/5znciIqKsrCwuuuiimDZtWjRs2DB22mmnePDBBwuHDV511VWxxRZb/NeTTgAAAHxTX/vsfPVZZWVllJeXOzsfrDH19wUQZ+erm5ydr25ydr66ydn56iZn5wMAAOArE1EAAAAZiCgAAIAMRBQAAEAGIgoAACADEQUAAJCBiAIAAMhARAEAAGQgogAAADIQUQAAABmIKAAAgAxEFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADBrlPUBtlFKKiIjqVB1RnfMw8K2Q8h6gaFY8ntRP9Xfd6vf9Vn9VV9ffJ+1ly5bmPULRVFcvy3uEolm6dEneIxRNff17W7Fe/+15oCR5pljJe++9F+3bt897DAAAIAczZ86Mdu3afelyEbUK1dXVMWvWrGjevHmUlJQU9d+qrKyM9u3bx8yZM6NFixZF/bfWNOtWN1m3usm61U3WrW6ybnWTdaub1vS6pZRi/vz5sdFGG0WDBl/+zieH861CgwYN/mN5FkOLFi3q3S/9CtatbrJudZN1q5usW91k3eom61Y3rcl1Ky8v/6/XcWIJAACADEQUAABABiIqZ6WlpfGb3/wmSktL8x5ltbNudZN1q5usW91k3eom61Y3Wbe6qbaumxNLAAAAZGBPFAAAQAYiCgAAIAMRBQAAkIGIAgAAyEBEAQAAZCCiAAAAMhBRAAAAGYgoAACADP4fBQ2dWET/UPYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
