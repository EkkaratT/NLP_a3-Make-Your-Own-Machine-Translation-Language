{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('kvush/english_thai_texts')\n",
    "\n",
    "ENG_LANGUAGE = 'input_text'\n",
    "THAI_LANGUAGE = 'translated_text'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the dataset manually into train, validation, and test\n",
    "\n",
    "train_subset = dataset['train'].select(range(41901))   # Select first 70% rows for training\n",
    "\n",
    "validation_subset = dataset['train'].select(range(41901, 47887))   # Select next 10% rows for validation\n",
    "\n",
    "test_subset = dataset['train'].select(range(47887, 59859))   # Select next 20% rows for testing\n",
    "\n",
    "# Create a new DatasetDict with the desired subsets\n",
    "dataset = DatasetDict({\n",
    "    'train': train_subset,\n",
    "    'validation': validation_subset,\n",
    "    'test': test_subset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 41901\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 5986\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'input_ids', 'translated_text', 'translated_ids', '__index_level_0__'],\n",
       "        num_rows: 11972\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['validation']]\n",
    "test = [(row[ENG_LANGUAGE], row[THAI_LANGUAGE]) for row in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41901"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11972"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "token_transform[ENG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[THAI_LANGUAGE] = word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(token_transform, 'token_transform.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Service, scallops, all - top notch in every way!',\n",
       " 'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the English and Thai sentences\n",
    "english_sentence = sample[0]  # English part\n",
    "thai_sentence = sample[1]  # Thai part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence:  Service, scallops, all - top notch in every way!\n",
      "English Tokenization:  ['Service', ',', 'scallops', ',', 'all', '-', 'top', 'notch', 'in', 'every', 'way', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of English sentence\n",
    "print(\"English Sentence: \", english_sentence)\n",
    "print(\"English Tokenization: \", token_transform[ENG_LANGUAGE](english_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thai Sentence:  บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!\n",
      "Thai Tokenization:  ['บริการ', 'หอย', 'เชลล์', 'ทั้งหมด', ' ', '-', ' ', 'โดดเด่น', 'ใน', 'ทุก', 'ด้าน', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of Thai sentence\n",
    "print(\"Thai Sentence: \", thai_sentence)\n",
    "print(\"Thai Tokenization: \", token_transform[THAI_LANGUAGE](thai_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {ENG_LANGUAGE: 0, THAI_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocab_transform dictionary\n",
    "torch.save(vocab_transform, 'vocab_transform.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 12, 10, 0, 10]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[ENG_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haircut'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[ENG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16607"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [ENG_LANGUAGE, THAI_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[ENG_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[THAI_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 43])\n",
      "Thai shape:  torch.Size([64, 58])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim     #size of word embedding\n",
    "        self.n_heads  = n_heads      #head\n",
    "        self.head_dim = hid_dim // n_heads   #dk\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale      # from e = s.h   e is att or energy, s is Q(decoder hidden states,  queries), h is K(encoder hidden states, keys)\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0 \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[ENG_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[THAI_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16607, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(11664, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=11664, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[ENG_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[THAI_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4251392\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2985984\n",
      " 11664\n",
      "______\n",
      "14239888\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 45s\n",
      "\tTrain Loss: 4.956 | Train PPL: 142.062\n",
      "\t Val. Loss: 3.899 |  Val. PPL:  49.334\n",
      "Epoch: 02 | Time: 1m 46s\n",
      "\tTrain Loss: 3.429 | Train PPL:  30.858\n",
      "\t Val. Loss: 2.962 |  Val. PPL:  19.343\n",
      "Epoch: 03 | Time: 1m 45s\n",
      "\tTrain Loss: 2.656 | Train PPL:  14.233\n",
      "\t Val. Loss: 2.528 |  Val. PPL:  12.532\n",
      "Epoch: 04 | Time: 1m 46s\n",
      "\tTrain Loss: 2.177 | Train PPL:   8.815\n",
      "\t Val. Loss: 2.312 |  Val. PPL:  10.093\n",
      "Epoch: 05 | Time: 1m 49s\n",
      "\tTrain Loss: 1.845 | Train PPL:   6.327\n",
      "\t Val. Loss: 2.164 |  Val. PPL:   8.703\n",
      "Epoch: 06 | Time: 1m 47s\n",
      "\tTrain Loss: 1.603 | Train PPL:   4.969\n",
      "\t Val. Loss: 2.106 |  Val. PPL:   8.212\n",
      "Epoch: 07 | Time: 1m 47s\n",
      "\tTrain Loss: 1.424 | Train PPL:   4.153\n",
      "\t Val. Loss: 2.082 |  Val. PPL:   8.024\n",
      "Epoch: 08 | Time: 1m 47s\n",
      "\tTrain Loss: 1.274 | Train PPL:   3.574\n",
      "\t Val. Loss: 2.034 |  Val. PPL:   7.643\n",
      "Epoch: 09 | Time: 1m 47s\n",
      "\tTrain Loss: 1.154 | Train PPL:   3.171\n",
      "\t Val. Loss: 2.059 |  Val. PPL:   7.839\n",
      "Epoch: 10 | Time: 1m 48s\n",
      "\tTrain Loss: 1.052 | Train PPL:   2.863\n",
      "\t Val. Loss: 2.067 |  Val. PPL:   7.897\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.057 | Test PPL:   7.826 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Service, scallops, all - top notch in every way!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'บริการหอยเชลล์ทั้งหมด - โดดเด่นในทุกด้าน!'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  153,    6, 2910,    6,   64,   58,  508, 1441,   19,  250,  219,\n",
       "          11,    3], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[ENG_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   28, 1352, 2288,  168,    4,  150,    4,  661,   11,  326,  422,\n",
       "          13,    3], device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[THAI_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 11664])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 11664])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11664])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1352, 2288,  168,   11,  150,    4,  164,   11,  326,  422,   13,    3,\n",
       "          13], device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[THAI_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "หอย\n",
      "เชลล์\n",
      "ทั้งหมด\n",
      "ใน\n",
      "-\n",
      " \n",
      "สุดยอด\n",
      "ใน\n",
      "ทุก\n",
      "ด้าน\n",
      "!\n",
      "<eos>\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 14, 14])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 14])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Service',\n",
       " ',',\n",
       " 'scallops',\n",
       " ',',\n",
       " 'all',\n",
       " '-',\n",
       " 'top',\n",
       " 'notch',\n",
       " 'in',\n",
       " 'every',\n",
       " 'way',\n",
       " '!',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[ENG_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'หอย',\n",
       " 'เชลล์',\n",
       " 'ทั้งหมด',\n",
       " 'ใน',\n",
       " '-',\n",
       " ' ',\n",
       " 'สุดยอด',\n",
       " 'ใน',\n",
       " 'ทุก',\n",
       " 'ด้าน',\n",
       " '!',\n",
       " '<eos>',\n",
       " '!']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_32216\\59549304.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\Ekkar\\AppData\\Local\\Temp\\ipykernel_32216\\59549304.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3629 (\\N{THAI CHARACTER O ANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3618 (\\N{THAI CHARACTER YO YAK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3648 (\\N{THAI CHARACTER SARA E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3594 (\\N{THAI CHARACTER CHO CHANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3660 (\\N{THAI CHARACTER THANTHAKHAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3591 (\\N{THAI CHARACTER NGO NGU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3617 (\\N{THAI CHARACTER MO MA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3604 (\\N{THAI CHARACTER DO DEK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3651 (\\N{THAI CHARACTER SARA AI MAIMUAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3626 (\\N{THAI CHARACTER SO SUA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3640 (\\N{THAI CHARACTER SARA U}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Ekkar\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
